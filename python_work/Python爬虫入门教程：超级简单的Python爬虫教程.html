<!DOCTYPE html>
<html lang="zh-CN"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script type="text/javascript" async="" charset="utf-8" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/core.php"></script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="renderer" content="webkit">
<meta name="force-rendering" content="webkit">
<meta name="applicable-device" content="pc,mobile">
<meta name="MobileOptimized" content="width">
<meta name="HandheldFriendly" content="true">
<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
<meta name="format-detection" content="telephone=no">
<link rel="shortcut icon" href="http://c.biancheng.net/favicon.ico?v=1.6.68">
<link href="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/common.css" rel="stylesheet">
<title>Python爬虫入门教程：超级简单的Python爬虫教程</title>
<meta name="description" content="这是一篇从实战出发，面向 0 基础学员的 Python 爬虫入门教程，只要耐心读完本文，30 分钟即可学会编写简单的 Python 爬虫。 本篇 Python 爬虫教程主要讲解了解网页、使用 requests 库抓取网">
</head>
<body>
<div id="topbar" class="clearfix">
<ul id="product-type" class="left">
<li>
<a href="http://c.biancheng.net/"><span class="iconfont iconfont-home"></span>首页</a>
</li>
<li class="active">
<a href="http://c.biancheng.net/sitemap/" rel="nofollow"><span class="iconfont iconfont-book"></span>教程</a>
</li>
<li>
<a href="http://vip.biancheng.net/p/vip/show.php" rel="nofollow" target="_blank"><span class="iconfont iconfont-vip"></span>VIP会员</a>
</li>
<li>
<a href="http://vip.biancheng.net/p/q2a/show.php" rel="nofollow" target="_blank"><span class="iconfont iconfont-q2a"></span>一对一答疑</a>
</li>
<li>
<a href="http://fudao.biancheng.net/" rel="nofollow" target="_blank"><span class="iconfont iconfont-fudao"></span>辅导班</a>
</li>
</ul>
</div>
<div id="header" class="clearfix">
<a id="logo" class="left" href="http://c.biancheng.net/">
<img src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/logo.png" alt="C语言中文网" height="26">
</a>
<ul id="nav-main" class="hover-none left clearfix">
<li class="wap-yes"><a href="http://c.biancheng.net/">首页</a></li>
<li><a href="http://c.biancheng.net/c/">C语言教程</a></li>
<li><a href="http://c.biancheng.net/cplus/">C++教程</a></li>
<li><a href="http://c.biancheng.net/python/">Python教程</a></li>
<li><a href="http://c.biancheng.net/java/">Java教程</a></li>
<li><a href="http://c.biancheng.net/linux_tutorial/">Linux入门</a></li>
<li><a href="http://c.biancheng.net/sitemap/" title="网站地图">更多&gt;&gt;</a></li>
</ul>
<a href="http://vip.biancheng.net/?from=topbar" class="user-info glyphicon glyphicon-user hover-none" target="_blank" rel="nofollow" title="用户中心"></a>
</div>
<div id="main-no-course" class="clearfix">
<div class="arc-info">
<span class="position"><span class="glyphicon glyphicon-map-marker"></span> <a href="http://c.biancheng.net/">首页</a> &gt; <a href="http://c.biancheng.net/skill/">编程笔记</a> &gt; <a href="http://c.biancheng.net/skill/python/">Python笔记</a></span>
<span class="read-num">阅读：465,931</span>
</div>
<div id="ad-position-bottom"></div>
<h1>Python爬虫入门教程：超级简单的Python爬虫教程</h1>
<div id="ad-arc-top"><p class="pic"></p><p class="text" adid="python-1v1q2a"><a href="http://c.biancheng.net/view/7548.html" target="_blank" style="color: #D33428;">Python一对一答疑，帮助有志青年！使用QQ在线辅导，哪里不懂问哪里，整个过程都是一对一，学习更有针对性。和作者直接交流，不但提升技能，还提升 Level；当你决定加入我们，你已然超越了 90% 的程序员。<span style="color:#08c;">猛击这里了解详情。</span></a></p></div>
<div id="arc-body">这是一篇详细介绍 <a href="http://c.biancheng.net/python/" target="_blank">Python</a> 爬虫入门的教程，从实战出发，适合初学者。读者只需在阅读过程紧跟文章思路，理清相应的实现代码，30 分钟即可学会编写简单的 Python 爬虫。<br>
<br>
这篇 Python 爬虫教程主要讲解以下 5 部分内容：
<ol>
<li>
了解网页；</li>
<li>
使用 requests 库抓取网站数据；</li>
<li>
使用 Beautiful Soup 解析网页；</li>
<li>
清洗和组织数据；</li>
<li>
爬虫攻防战；</li>
</ol>
<h2>
了解网页</h2>
以中国旅游网首页（<a href="http://www.cntour.cn/" target="_blank">http://www.cntour.cn/</a>）为例，抓取中国旅游网首页首条信息（标题和链接），数据以明文的形式出面在源码中。在中国旅游网首页，按快捷键【Ctrl+U】打开源码页面，如图 1 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G63943S6.jpg"><br>
图 1 中国旅游网首页源码</div>
<h3>
认识网页结构</h3>
网页一般由三部分组成，分别是 HTML（超文本标记语言）、CSS（层叠样式表）和 JScript（活动脚本语言）。<br>
<h4>
HTML</h4>
HTML 是整个网页的结构，相当于整个网站的框架。带“＜”、“＞”符号的都是属于 HTML 的标签，并且标签都是成对出现的。<br>
<br>
常见的标签如下：<br>
<p class="info-box">
&lt;html&gt;..&lt;/html&gt; 表示标记中间的元素是网页<br>
&lt;body&gt;..&lt;/body&gt; 表示用户可见的内容<br>
&lt;div&gt;..&lt;/div&gt; 表示框架<br>
&lt;p&gt;..&lt;/p&gt; 表示段落<br>
&lt;li&gt;..&lt;/li&gt;表示列表<br>
&lt;img&gt;..&lt;/img&gt;表示图片<br>
&lt;h1&gt;..&lt;/h1&gt;表示标题<br>
&lt;a href=""&gt;..&lt;/a&gt;表示超链接</p>
<h4>
CSS</h4>
CSS 表示样式，图 1 中第 13 行＜style type=＂text/css＂＞表示下面引用一个 CSS，在 CSS 中定义了外观。<br>
<h4>
JScript</h4>
JScript 表示功能。交互的内容和各种特效都在 JScript 中，JScript 描述了网站中的各种功能。<br>
<br>
如果用人体来比喻，HTML 是人的骨架，并且定义了人的嘴巴、眼睛、耳朵等要长在哪里。CSS 是人的外观细节，如嘴巴长什么样子，眼睛是双眼皮还是单眼皮，是大眼睛还是小眼睛，皮肤是黑色的还是白色的等。JScript 表示人的技能，例如跳舞、唱歌或者演奏乐器等。<br>
<h3>
写一个简单的 HTML</h3>
通过编写和修改 HTML，可以更好地理解 HTML。首先打开一个记事本，然后输入下面的内容：<br>
<p class="info-box">
&lt;html&gt;<br>
&lt;head&gt;<br>
&nbsp;&nbsp;&nbsp; &lt;title&gt; Python 3 爬虫与数据清洗入门与实战&lt;/title&gt;<br>
&lt;/head&gt;<br>
&lt;body&gt;<br>
&nbsp;&nbsp;&nbsp; &lt;div&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;p&gt;Python 3爬虫与数据清洗入门与实战&lt;/p&gt;<br>
&nbsp;&nbsp;&nbsp; &lt;/div&gt;<br>
&nbsp;&nbsp;&nbsp; &lt;div&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;ul&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
&lt;li&gt;&lt;a href="http://c.biancheng.net"&gt;爬虫&lt;/a&gt;&lt;/li&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;li&gt;数据清洗&lt;/li&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;/ul&gt;<br>
&nbsp;&nbsp;&nbsp; &lt;/div&gt;<br>
&lt;/body&gt;</p>
输入代码后，保存记事本，然后修改文件名和后缀名为"HTML.html"；<br>
<br>
运行该文件后的效果，如图 2 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G64134V2.gif"><br>
图 2</div>
<br>
这段代码只是用到了 HTML，读者可以自行修改代码中的中文，然后观察其变化。<br>
<h3>
关于爬虫的合法性</h3>
几乎每一个网站都有一个名为 robots.txt 的文档，当然也有部分网站没有设定 robots.txt。对于没有设定 robots.txt 
的网站可以通过网络爬虫获取没有口令加密的数据，也就是该网站所有页面数据都可以爬取。如果网站有 robots.txt 
文档，就要判断是否有禁止访客获取的数据。<br>
<br>
以淘宝网为例，在浏览器中访问 <a href="https://www.taobao.com/robots.txt" target="_blank">https://www.taobao.com/robots.txt</a>，如图&nbsp; 3 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G6422Ub.gif"><br>
图 3 淘宝网的robots.txt文件内容</div>
<br>
淘宝网允许部分爬虫访问它的部分路径，而对于没有得到允许的用户，则全部禁止爬取，代码如下：<br>
<p class="info-box">
User-Agent:*<br>
Disallow:/</p>
这一句代码的意思是除前面指定的爬虫外，不允许其他爬虫爬取任何数据。<br>
<h2>
使用 requests 库请求网站</h2>
<h3>
安装 requests 库</h3>
首先在 PyCharm 中安装 requests 库，为此打开 PyCharm，单击“File”（文件）菜单，选择“Setting for New Projects...”命令，如图 4 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G64314Q6.jpg"><br>
图 4</div>
<br>
选择“Project Interpreter”（项目编译器）命令，确认当前选择的编译器，然后单击右上角的加号，如图 5 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G64341W4.jpg"><br>
图 5</div>
<br>
在搜索框输入：requests（注意，一定要输入完整，不然容易出错），然后单击左下角的“Install Package”（安装库）按钮。如图 6 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G64400193.jpg"><br>
图 6</div>
<br>
安装完成后，会在 Install Package 上显示“Package‘requests’ installed successfully”（库的请求已成功安装），如图 7 所示；如果安装不成功将会显示提示信息。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G644192R.jpg"><br>
图 7 安装成功</div>
<h3>
爬虫的基本原理</h3>
网页请求的过程分为两个环节：
<ol>
<li>
Request （请求）：每一个展示在用户面前的网页都必须经过这一步，也就是向服务器发送访问请求。</li>
<li>
Response（响应）：服务器在接收到用户的请求后，会验证请求的有效性，然后向用户（客户端）发送响应的内容，客户端接收服务器响应的内容，将内容展示出来，就是我们所熟悉的网页请求，如图 8 所示。</li>
</ol>
<div style="text-align: center;">
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G6451I08.gif"><br>
图 8 Response相应</div>
<br>
网页请求的方式也分为两种：
<ol>
<li>
GET：最常见的方式，一般用于获取或者查询资源信息，也是大多数网站使用的方式，响应速度快。</li>
<li>
POST：相比 GET 方式，多了以表单形式上传参数的功能，因此除查询信息外，还可以修改信息。</li>
</ol>
<br>
所以，在写爬虫前要先确定向谁发送请求，用什么方式发送。<br>
<h3>
使用 GET 方式抓取数据</h3>
复制任意一条首页首条新闻的标题，在源码页面按【Ctrl+F】组合键调出搜索框，将标题粘贴在搜索框中，然后按【Enter】键。<br>
<br>
如图 8 所示，标题可以在源码中搜索到，请求对象是www.cntour.cn，请求方式是GET（所有在源码中的数据请求方式都是GET），如图 9 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G64620C9.jpg"><br>
图 9（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11G6464b94.jpg" target="_blank">点此查看高清大图</a>）</div>
确定好请求对象和方式后，在 PyCharm 中输入以下代码：
<div class="snippet-container" style="undefined;"><div class="sh_bright snippet-wrap"><div class="snippet-menu sh_sourceCode" style="display:none;"><pre><a class="snippet-copy sh_url" href="#" style="display: none;">复制</a><a class="snippet-text sh_url" href="#">纯文本</a><a class="snippet-window sh_url" href="#">复制</a></pre></div><pre class="python sh_python snippet-formatted sh_sourceCode"><ol class="snippet-num"><li><span class="sh_preproc">import</span> requests        <span class="sh_comment">#导入requests包</span></li><li>url <span class="sh_symbol">=</span> <span class="sh_string">'http://www.cntour.cn/'</span></li><li>strhtml <span class="sh_symbol">=</span> requests<span class="sh_symbol">.</span><span class="sh_function">get</span><span class="sh_symbol">(</span>url<span class="sh_symbol">)</span>        <span class="sh_comment">#Get方式获取网页数据</span></li><li><span class="sh_keyword">print</span><span class="sh_symbol">(</span>strhtml<span class="sh_symbol">.</span>text<span class="sh_symbol">)</span></li></ol></pre><pre class="snippet-textonly sh_sourceCode" style="display:none;">import requests        #导入requests包
url = 'http://www.cntour.cn/'
strhtml = requests.get(url)        #Get方式获取网页数据
print(strhtml.text)</pre></div></div>
运行结果如图 10 所示：
<div style="text-align: center;">
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G64I4104.jpg"><br>
图 10 运行结果效果图（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11G64K5445.jpg" target="_blank">点此查看高清大图</a>）</div>
<br>
加载库使用的语句是 import+库的名字。在上述过程中，加载 requests 库的语句是：import requests。<br>
<br>
用 GET 方式获取数据需要调用 requests 库中的 get 方法，使用方法是在 requests 后输入英文点号，如下所示：<br>
<p class="info-box">
requests.get</p>
将获取到的数据存到 strhtml 变量中，代码如下：
<p class="info-box">
strhtml = request.get(url)</p>
这个时候 strhtml 是一个 URL 对象，它代表整个网页，但此时只需要网页中的源码，下面的语句表示网页源码：
<p class="info-box">
strhtml.text</p>
<h3>
使用 POST 方式抓取数据</h3>
首先输入有道翻译的网址：<a href="http://fanyi.youdao.com/" target="_blank">http://fanyi.youdao.com/</a>，进入有道翻译页面。<br>
<br>
按快捷键 F12，进入开发者模式，单击 Network，此时内容为空，如图 11 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G64ZU55.gif"><br>
图 11</div>
<br>
在有道翻译中输入“我爱中国”，单击“翻译”按钮，如图 12 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G6492K96.gif"><br>
图 12</div>
<br>
在开发者模式中，依次单击“Network”按钮和“XHR”按钮，找到翻译数据，如图 13 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11G6494Y00.gif"><br>
图 13</div>
<br>
单击 Headers，发现请求数据的方式为 POST。如图 14 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GA00I57.gif"><br>
图 14</div>
<br>
找到数据所在之处并且明确请求方式之后，接下来开始撰写爬虫。<br>
<br>
首先，将 Headers 中的 URL 复制出来，并赋值给 url，代码如下：<br>
<p class="info-box">
url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'</p>
POST 的请求获取数据的方式不同于 GET，POST 请求数据必须构建请求头才可以。<br>
<br>
Form Data 中的请求参数如图 15 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GA041451.gif"><br>
图 15</div>
<br>
将其复制并构建一个新字典：
<p class="info-box">
From_data={'i':'我愛中國','from':'zh-
CHS','to':'en','smartresult':'dict','client':'fanyideskweb','salt':'15477056211258','sign':'b3589f32c38bc9e3876a570b8a992604','ts':'1547705621125','bv':'b33a2f3f9d09bde064c9275bcb33d94e','doctype':'json','version':'2.1','keyfrom':'fanyi.web','action':'FY_BY_REALTIME','typoResult':'false'}</p>
接下来使用 requests.post 方法请求表单数据，代码如下：
<p class="info-box">
import requests&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #导入requests包<br>
response = requests.post(url,data=payload)</p>
将字符串格式的数据转换成 JSON 格式数据，并根据<a href="http://c.biancheng.net/data_structure/" target="_blank">数据结构</a>，提取数据，并将翻译结果打印出来，代码如下：<br>
<div class="snippet-container" style="undefined;"><div class="sh_bright snippet-wrap"><div class="snippet-menu sh_sourceCode" style="display:none;"><pre><a class="snippet-copy sh_url" href="#" style="display: none;">复制</a><a class="snippet-text sh_url" href="#">纯文本</a><a class="snippet-window sh_url" href="#">复制</a></pre></div><pre class="python sh_python snippet-formatted sh_sourceCode"><ol class="snippet-num"><li><span class="sh_preproc">import</span> json</li><li>content <span class="sh_symbol">=</span> json<span class="sh_symbol">.</span><span class="sh_function">loads</span><span class="sh_symbol">(</span>response<span class="sh_symbol">.</span>text<span class="sh_symbol">)</span></li><li><span class="sh_keyword">print</span><span class="sh_symbol">(</span>content<span class="sh_symbol">[</span><span class="sh_string">'translateResult'</span><span class="sh_symbol">][</span><span class="sh_number">0</span><span class="sh_symbol">][</span><span class="sh_number">0</span><span class="sh_symbol">][</span><span class="sh_string">'tgt'</span><span class="sh_symbol">])</span></li></ol></pre><pre class="snippet-textonly sh_sourceCode" style="display:none;">import json
content = json.loads(response.text)
print(content['translateResult'][0][0]['tgt'])</pre></div></div>
使用 requests.post 方法抓取有道翻译结果的完整代码如下：<br>
<div class="snippet-container" style="undefined;"><div class="sh_bright snippet-wrap"><div class="snippet-menu sh_sourceCode" style="display:none;"><pre><a class="snippet-copy sh_url" href="#" style="display: none;">复制</a><a class="snippet-text sh_url" href="#">纯文本</a><a class="snippet-window sh_url" href="#">复制</a></pre></div><pre class="python sh_python snippet-formatted sh_sourceCode"><ol class="snippet-num"><li><span class="sh_preproc">import</span> requests        <span class="sh_comment">#导入requests包</span></li><li><span class="sh_preproc">import</span> json</li><li><span class="sh_keyword">def</span> <span class="sh_function">get_translate_date</span><span class="sh_symbol">(</span>word<span class="sh_symbol">=</span>None<span class="sh_symbol">):</span></li><li>    url <span class="sh_symbol">=</span> <span class="sh_string">'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'</span></li><li>    From_data<span class="sh_symbol">={</span><span class="sh_string">'i'</span><span class="sh_symbol">:</span>word<span class="sh_symbol">,</span><span class="sh_string">'from'</span><span class="sh_symbol">:</span><span class="sh_string">'zh-CHS'</span><span class="sh_symbol">,</span><span class="sh_string">'to'</span><span class="sh_symbol">:</span><span class="sh_string">'en'</span><span class="sh_symbol">,</span><span class="sh_string">'smartresult'</span><span class="sh_symbol">:</span><span class="sh_string">'dict'</span><span class="sh_symbol">,</span><span class="sh_string">'client'</span><span class="sh_symbol">:</span><span class="sh_string">'fanyideskweb'</span><span class="sh_symbol">,</span><span class="sh_string">'salt'</span><span class="sh_symbol">:</span><span class="sh_string">'15477056211258'</span><span class="sh_symbol">,</span><span class="sh_string">'sign'</span><span class="sh_symbol">:</span><span class="sh_string">'b3589f32c38bc9e3876a570b8a992604'</span><span class="sh_symbol">,</span><span class="sh_string">'ts'</span><span class="sh_symbol">:</span><span class="sh_string">'1547705621125'</span><span class="sh_symbol">,</span><span class="sh_string">'bv'</span><span class="sh_symbol">:</span><span class="sh_string">'b33a2f3f9d09bde064c9275bcb33d94e'</span><span class="sh_symbol">,</span><span class="sh_string">'doctype'</span><span class="sh_symbol">:</span><span class="sh_string">'json'</span><span class="sh_symbol">,</span><span class="sh_string">'version'</span><span class="sh_symbol">:</span><span class="sh_string">'2.1'</span><span class="sh_symbol">,</span><span class="sh_string">'keyfrom'</span><span class="sh_symbol">:</span><span class="sh_string">'fanyi.web'</span><span class="sh_symbol">,</span><span class="sh_string">'action'</span><span class="sh_symbol">:</span><span class="sh_string">'FY_BY_REALTIME'</span><span class="sh_symbol">,</span><span class="sh_string">'typoResult'</span><span class="sh_symbol">:</span><span class="sh_string">'false'</span><span class="sh_symbol">}</span></li><li>    <span class="sh_comment">#请求表单数据</span></li><li>    response <span class="sh_symbol">=</span> requests<span class="sh_symbol">.</span><span class="sh_function">post</span><span class="sh_symbol">(</span>url<span class="sh_symbol">,</span>data<span class="sh_symbol">=</span>From_data<span class="sh_symbol">)</span></li><li>    <span class="sh_comment">#将Json格式字符串转字典</span></li><li>    content <span class="sh_symbol">=</span> json<span class="sh_symbol">.</span><span class="sh_function">loads</span><span class="sh_symbol">(</span>response<span class="sh_symbol">.</span>text<span class="sh_symbol">)</span></li><li>    <span class="sh_keyword">print</span><span class="sh_symbol">(</span>content<span class="sh_symbol">)</span></li><li>    <span class="sh_comment">#打印翻译后的数据</span></li><li>    <span class="sh_comment">#print(content['translateResult'][0][0]['tgt'])</span></li><li><span class="sh_keyword">if</span> __name__<span class="sh_symbol">==</span><span class="sh_string">'__main__'</span><span class="sh_symbol">:</span></li><li>    <span class="sh_function">get_translate_date</span><span class="sh_symbol">(</span><span class="sh_string">'我爱中国'</span><span class="sh_symbol">)</span></li></ol></pre><pre class="snippet-textonly sh_sourceCode" style="display:none;">import requests        #导入requests包
import json
def get_translate_date(word=None):
    url = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'
    From_data={'i':word,'from':'zh-CHS','to':'en','smartresult':'dict','client':'fanyideskweb','salt':'15477056211258','sign':'b3589f32c38bc9e3876a570b8a992604','ts':'1547705621125','bv':'b33a2f3f9d09bde064c9275bcb33d94e','doctype':'json','version':'2.1','keyfrom':'fanyi.web','action':'FY_BY_REALTIME','typoResult':'false'}
    #请求表单数据
    response = requests.post(url,data=From_data)
    #将Json格式字符串转字典
    content = json.loads(response.text)
    print(content)
    #打印翻译后的数据
    #print(content['translateResult'][0][0]['tgt'])
if __name__=='__main__':
    get_translate_date('我爱中国')</pre></div></div>
<h2>
使用 Beautiful Soup 解析网页</h2>
通过 requests 库已经可以抓到网页源码，接下来要从源码中找到并提取数据。Beautiful Soup 是 python 
的一个库，其最主要的功能是从网页中抓取数据。Beautiful Soup 目前已经被移植到 bs4 库中，也就是说在导入 Beautiful 
Soup 时需要先安装 bs4 库。<br>
<br>
安装 bs4 库的方式如图 16 所示:
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GA15bD.gif"><br>
图 16</div>
<br>
安装好 bs4 库以后，还需安装 lxml 库。如果我们不安装 lxml 库，就会使用 Python 默认的解析器。尽管 Beautiful 
Soup 既支持 Python 标准库中的 HTML 解析器又支持一些第三方解析器，但是 lxml 
库具有功能更加强大、速度更快的特点，因此笔者推荐安装 lxml 库。<br>
<br>
安装 Python 第三方库后，输入下面的代码，即可开启 Beautiful Soup 之旅：<br>
<div class="snippet-container" style="undefined;"><div class="sh_bright snippet-wrap"><div class="snippet-menu sh_sourceCode" style="opacity: 0.360504;"><pre><a class="snippet-copy sh_url" href="#" style="display: none;">复制</a><a class="snippet-text sh_url" href="#">纯文本</a><a class="snippet-window sh_url" href="#">复制</a></pre></div><pre class="python sh_python snippet-formatted sh_sourceCode"><ol class="snippet-num"><li><span class="sh_preproc">import</span> requests        <span class="sh_comment">#导入requests包</span></li><li><span class="sh_preproc">from</span> bs4 <span class="sh_preproc">import</span>    BeautifulSoup</li><li>url<span class="sh_symbol">=</span><span class="sh_string">'http://www.cntour.cn/'</span></li><li>strhtml<span class="sh_symbol">=</span>requests<span class="sh_symbol">.</span><span class="sh_function">get</span><span class="sh_symbol">(</span>url<span class="sh_symbol">)</span></li><li>soup<span class="sh_symbol">=</span><span class="sh_function">BeautifulSoup</span><span class="sh_symbol">(</span>strhtml<span class="sh_symbol">.</span>text<span class="sh_symbol">,</span><span class="sh_string">'lxml'</span><span class="sh_symbol">)</span></li><li>data <span class="sh_symbol">=</span> soup<span class="sh_symbol">.</span><span class="sh_function">select</span><span class="sh_symbol">(</span><span class="sh_string">'#main&gt;div&gt;div.mtop.firstMod.clearfix&gt;div.centerBox&gt;ul.newsList&gt;li&gt;a'</span><span class="sh_symbol">)</span></li><li><span class="sh_keyword">print</span><span class="sh_symbol">(</span>data<span class="sh_symbol">)</span></li></ol></pre><pre class="snippet-textonly sh_sourceCode" style="display:none;">import requests        #导入requests包
from bs4 import    BeautifulSoup
url='http://www.cntour.cn/'
strhtml=requests.get(url)
soup=BeautifulSoup(strhtml.text,'lxml')
data = soup.select('#main&gt;div&gt;div.mtop.firstMod.clearfix&gt;div.centerBox&gt;ul.newsList&gt;li&gt;a')
print(data)</pre></div></div>
代码运行结果如图 17 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GA244Q8.jpg"><br>
图 17（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11GA30cY.jpg" target="_blank">点此查看高清大图</a>）</div>
<br>
Beautiful Soup 库能够轻松解析网页信息，它被集成在 bs4 库中，需要时可以从 bs4 库中调用。其表达语句如下：
<p class="info-box">
from bs4 import BeautifulSoup</p>
首先，HTML 文档将被转换成 Unicode 编码格式，然后 Beautiful Soup 选择最合适的解析器来解析这段文档，此处指定 
lxml 解析器进行解析。解析后便将复杂的 HTML 文档转换成树形结构，并且每个节点都是 Python 
对象。这里将解析后的文档存储到新建的变量 soup 中，代码如下：<br>
<p class="info-box">
soup=BeautifulSoup(strhtml.text,'lxml')</p>
接下来用 select（选择器）定位数据，定位数据时需要使用浏览器的开发者模式，将鼠标光标停留在对应的数据位置并右击，然后在快捷菜单中选择“检查”命令，如图 18 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GA40T17.gif"><br>
图 18</div>
<br>
随后在浏览器右侧会弹出开发者界面，右侧高亮的代码（参见图&nbsp; 19(b)）对应着左侧高亮的数据文本（参见图 19(a)）。右击右侧高亮数据，在弹出的快捷菜单中选择“Copy”➔“Copy Selector”命令，便可以自动复制路径。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GA45JN.gif"><br>
图 19 复制路径</div>
将路径粘贴在文档中，代码如下:<br>
<p class="info-box">
#main &gt; div &gt; div.mtop.firstMod.clearfix &gt; div.centerBox &gt; ul.newsList &gt; li:nth-child(1) &gt; a</p>
由于这条路径是选中的第一条的路径，而我们需要获取所有的头条新闻，因此将 li：nth-child（1）中冒号（包含冒号）后面的部分删掉，代码如下：
<p class="info-box">
#main &gt; div &gt; div.mtop.firstMod.clearfix &gt; div.centerBox &gt; ul.newsList &gt; li &gt; a</p>
使用 soup.select 引用这个路径，代码如下：<br>
<p class="info-box">
data = soup.select('#main &gt; div &gt; div.mtop.firstMod.clearfix &gt; div.centerBox &gt; ul.newsList &gt; li &gt; a')</p>
<h2>
清洗和组织数据</h2>
至此，获得了一段目标的 HTML 代码，但还没有把数据提取出来，接下来在 PyCharm 中输入以下代码：<br>
<div class="snippet-container" style="undefined;"><div class="sh_bright snippet-wrap"><div class="snippet-menu sh_sourceCode" style="display:none;"><pre><a class="snippet-copy sh_url" href="#" style="display: none;">复制</a><a class="snippet-text sh_url" href="#">纯文本</a><a class="snippet-window sh_url" href="#">复制</a></pre></div><pre class="python sh_python snippet-formatted sh_sourceCode"><ol class="snippet-num"><li><span class="sh_keyword">for</span> item <span class="sh_keyword">in</span> data<span class="sh_symbol">:</span></li><li>    result<span class="sh_symbol">={</span></li><li>        <span class="sh_string">'title'</span><span class="sh_symbol">:</span>item<span class="sh_symbol">.</span><span class="sh_function">get_text</span><span class="sh_symbol">(),</span></li><li>        <span class="sh_string">'link'</span><span class="sh_symbol">:</span>item<span class="sh_symbol">.</span><span class="sh_function">get</span><span class="sh_symbol">(</span><span class="sh_string">'href'</span><span class="sh_symbol">)</span></li><li>    <span class="sh_symbol">}</span></li><li><span class="sh_keyword">print</span><span class="sh_symbol">(</span>result<span class="sh_symbol">)</span></li></ol></pre><pre class="snippet-textonly sh_sourceCode" style="display:none;">for item in data:
    result={
        'title':item.get_text(),
        'link':item.get('href')
    }
print(result)</pre></div></div>
代码运行结果如图 20 所示：
<div style="text-align: center;">
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GA60R06.gif"><br>
图 20（<a href="http://c.biancheng.net/uploads/allimg/190117/2-1Z11GA629403.jpg" target="_blank">点此查看高清大图</a>）</div>
<br>
首先明确要提取的数据是标题和链接，标题在＜a＞标签中，提取标签的正文用 get_text() 方法。链接在＜a＞标签的 href 属性中，提取标签中的 href 属性用 get() 方法，在括号中指定要提取的属性数据，即 get(＇href＇)。<br>
<br>
从图 20 中可以发现，文章的链接中有一个数字 ID。下面用正则表达式提取这个 ID。需要使用的正则符号如下:
<p class="info-box">
\d匹配数字<br>
+匹配前一个字符1次或多次</p>
在 Python 中调用正则表达式时使用 re 库，这个库不用安装，可以直接调用。在 PyCharm 中输入以下代码:
<div class="snippet-container" style="undefined;"><div class="sh_bright snippet-wrap"><div class="snippet-menu sh_sourceCode" style="display:none;"><pre><a class="snippet-copy sh_url" href="#" style="display: none;">复制</a><a class="snippet-text sh_url" href="#">纯文本</a><a class="snippet-window sh_url" href="#">复制</a></pre></div><pre class="python sh_python snippet-formatted sh_sourceCode"><ol class="snippet-num"><li><span class="sh_preproc">import</span> re</li><li><span class="sh_keyword">for</span> item <span class="sh_keyword">in</span> data<span class="sh_symbol">:</span></li><li>    result<span class="sh_symbol">={</span></li><li>        <span class="sh_string">"title"</span><span class="sh_symbol">:</span>item<span class="sh_symbol">.</span><span class="sh_function">get_text</span><span class="sh_symbol">(),</span></li><li>        <span class="sh_string">"link"</span><span class="sh_symbol">:</span>item<span class="sh_symbol">.</span><span class="sh_function">get</span><span class="sh_symbol">(</span><span class="sh_string">'href'</span><span class="sh_symbol">),</span></li><li>        <span class="sh_string">'ID'</span><span class="sh_symbol">:</span>re<span class="sh_symbol">.</span><span class="sh_function">findall</span><span class="sh_symbol">(</span><span class="sh_string">'\d+'</span><span class="sh_symbol">,</span>item<span class="sh_symbol">.</span><span class="sh_function">get</span><span class="sh_symbol">(</span><span class="sh_string">'href'</span><span class="sh_symbol">))</span></li><li>    <span class="sh_symbol">}</span></li><li><span class="sh_keyword">print</span><span class="sh_symbol">(</span>result<span class="sh_symbol">)</span></li></ol></pre><pre class="snippet-textonly sh_sourceCode" style="display:none;">import re
for item in data:
    result={
        "title":item.get_text(),
        "link":item.get('href'),
        'ID':re.findall('\d+',item.get('href'))
    }
print(result)</pre></div></div>
运行结果如图 21 所示：
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GAHKD.jpg"><br>
图 21</div>
<br>
这里使用 re 库的 findall 方法，第一个参数表示正则表达式，第二个参数表示要提取的文本。<br>
<h2>
爬虫攻防战</h2>
爬虫是模拟人的浏览访问行为，进行数据的批量抓取。当抓取的数据量逐渐增大时，会给被访问的服务器造成很大的压力，甚至有可能崩溃。换句话就是说，服务器是不喜欢有人抓取自己的数据的。那么，网站方面就会针对这些爬虫者，采取一些反爬策略。<br>
<br>
服务器第一种识别爬虫的方式就是通过检查连接的 useragent 来识别到底是浏览器访问，还是代码访问的。如果是代码访问的话，访问量增大时，服务器会直接封掉来访 IP。<br>
<br>
那么应对这种初级的反爬机制，我们应该采取何种举措？<br>
<br>
还是以前面创建好的爬虫为例。在进行访问时，我们在开发者环境下不仅可以找到 URL、Form Data，还可以在 Request headers 
中构造浏览器的请求头，封装自己。服务器识别浏览器访问的方法就是判断 keyword 是否为 Request headers 下的 
User-Agent，如图 22 所示。
<div style="text-align: center;">
<br>
<img alt="" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/2-1Z11GAR3P1.jpg"><br>
图 22</div>
<br>
因此，我们只需要构造这个请求头的参数。创建请求头部信息即可，代码如下：<br>
<p class="info-box">
headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) 
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 
Safari/537.36'}<br>
response = request.get(url,headers=headers)</p>
写到这里，很多读者会认为修改 User-Agent 很太简单。确实很简单，但是正常人1秒看一个图，而个爬虫1秒可以抓取好多张图，比如 1 
秒抓取上百张图，那么服务器的压力必然会增大。也就是说，如果在一个 IP 下批量访问下载图片，这个行为不符合正常人类的行为，肯定要被封 IP。<br>
<br>
其原理也很简单，就是统计每个IP的访问频率，该频率超过阈值，就会返回一个验证码，如果真的是用户访问的话，用户就会填写，然后继续访问，如果是代码访问的话，就会被封 IP。<br>
<br>
这个问题的解决方案有两个，第一个就是常用的增设延时，每 3 秒钟抓取一次，代码如下：
<p class="info-box">
import time<br>
time.sleep(3)</p>
但是，我们写爬虫的目的是为了高效批量抓取数据，这里设置 3 秒钟抓取一次，效率未免太低。其实，还有一个更重要的解决办法，那就是从本质上解决问题。<br>
<br>
不管如何访问，服务器的目的就是查出哪些为代码访问，然后封锁 IP。解决办法：为避免被封 IP，在数据采集时经常会使用代理。当然，requests 也有相应的 proxies 属性。<br>
<br>
首先，构建自己的代理 IP 池，将其以字典的形式赋值给 proxies，然后传输给 requests，代码如下：<br>
<div class="snippet-container" style="undefined;"><div class="sh_bright snippet-wrap"><div class="snippet-menu sh_sourceCode" style="display:none;"><pre><a class="snippet-copy sh_url" href="#" style="display: none;">复制</a><a class="snippet-text sh_url" href="#">纯文本</a><a class="snippet-window sh_url" href="#">复制</a></pre></div><pre class="python sh_python snippet-formatted sh_sourceCode"><ol class="snippet-num"><li>proxies<span class="sh_symbol">={</span></li><li>    <span class="sh_string">"http"</span><span class="sh_symbol">:</span><span class="sh_string">"http://10.10.1.10:3128"</span><span class="sh_symbol">,</span></li><li>    <span class="sh_string">"https"</span><span class="sh_symbol">:</span><span class="sh_string">"http://10.10.1.10:1080"</span><span class="sh_symbol">,</span></li><li><span class="sh_symbol">}</span></li><li>response <span class="sh_symbol">=</span> requests<span class="sh_symbol">.</span><span class="sh_function">get</span><span class="sh_symbol">(</span>url<span class="sh_symbol">,</span> proxies<span class="sh_symbol">=</span>proxies<span class="sh_symbol">)</span></li></ol></pre><pre class="snippet-textonly sh_sourceCode" style="display:none;">proxies={
    "http":"http://10.10.1.10:3128",
    "https":"http://10.10.1.10:1080",
}
response = requests.get(url, proxies=proxies)</pre></div></div>
<h2>
扩展阅读</h2>
本文仅对 Python 爬虫及实现过程做了简明扼要地介绍，仅能使初学者对 python 爬虫有一个浅显的认识，并不能让你完全掌握 Python 爬虫。如果您想全面的学习 Python 爬虫的相关知识，可以跳转至<a href="http://c.biancheng.net/python_spider/">《Python爬虫教程入门到精通》</a>进行学习。<br>
</div>
<div id="arc-append">
<p>关注微信公众号「魏雪原」，跟着<a class="col-link" href="http://c.biancheng.net/view/8092.html" target="_blank">站长</a>一起学习，拒绝躺平，跳出内卷，用知识完成阶级跃升：</p>
<ul>
<li>每日更新，坚持原创，敢说真话，凡事有态度。</li>
<li>编程+求职+创业+营销，既有硬知识，也有软技能。</li>
<li>手机端阅读教程，随时随地都能学习。</li>
<li>即将绑定网站部分功能，包括注册登录、找回密码、获取下载链接等。</li>
</ul>
<p style="margin-top:12px; text-align:center;">
<img src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/qrcode_weixueyuan_original.png" alt="魏雪原二维码" width="180"><br>
<span class="col-green">微信扫描二维码关注魏雪原</span>
</p>
</div>
<div id="ad-arc-bottom"></div>
<div id="nice-arcs" class="box-bottom">
<h4>优秀文章</h4>
<ul class="clearfix">
<li><a href="http://c.biancheng.net/view/178.html" title="for循环嵌套，C语言for循环嵌套详解">for循环嵌套，C语言for循环嵌套详解</a></li>
<li><a href="http://c.biancheng.net/view/423.html" title="C语言线程和进程">C语言线程和进程</a></li>
<li><a href="http://c.biancheng.net/view/505.html" title="C语言求回文数（详解版）">C语言求回文数（详解版）</a></li>
<li><a href="http://c.biancheng.net/view/3489.html" title="汇编语言操作数类型">汇编语言操作数类型</a></li>
<li><a href="http://c.biancheng.net/view/vip_5042.html" title="使用U盘安装Linux系统">使用U盘安装Linux系统</a></li>
<li><a href="http://c.biancheng.net/view/5512.html" title="JS while和do while循环语句">JS while和do while循环语句</a></li>
<li><a href="http://c.biancheng.net/view/7564.html" title="HTML表单：&lt;form&gt;标签">HTML表单：&lt;form&gt;标签</a></li>
<li><a href="http://c.biancheng.net/view/7816.html" title="MySQL设置日志输出方式">MySQL设置日志输出方式</a></li>
<li><a href="http://c.biancheng.net/spring/config-autowiring.html" title="Spring基于注解装配Bean">Spring基于注解装配Bean</a></li>
<li><a href="http://c.biancheng.net/jstl/fmt-bundle-message.html" title="&lt;fmt:bundle&gt;和&lt;fmt:message&gt;标签">&lt;fmt:bundle&gt;和&lt;fmt:message&gt;标签</a></li>
</ul>
</div>
</div>
<script type="text/javascript">
// 当前文章ID
window.arcIdRaw = 'a_' + 2011;
window.arcId = "27a6YTf+cGbcHPDLGPA/iHK3V4uszoJ2Xku0tq9aOeML7q0r39bXpggATqU";
window.typeidChain = "145|119";
</script>
<div id="footer" class="clearfix">
<div class="info left">
<p>精美而实用的网站，分享优质编程教程，帮助有志青年。千锤百炼，只为大作；精益求精，处处斟酌；这种教程，看一眼就倾心。</p>
<p>
<a href="http://c.biancheng.net/view/8066.html" target="_blank" rel="nofollow">关于网站</a> <span>|</span>
<a href="http://c.biancheng.net/view/8092.html" target="_blank" rel="nofollow">关于站长</a> <span>|</span>
<a href="http://c.biancheng.net/view/8097.html" target="_blank" rel="nofollow">如何完成一部教程</a> <span>|</span>
<a href="http://c.biancheng.net/view/8093.html" target="_blank" rel="nofollow">联系我们</a> <span>|</span>
<a href="http://c.biancheng.net/sitemap/" target="_blank" rel="nofollow">网站地图</a>
</p>
<p>Copyright ©2012-2021 biancheng.net, <a href="http://www.beian.miit.gov.cn/" target="_blank" rel="nofollow" style="color:#666;">陕ICP备15000209号</a></p>
</div>
<img class="right" src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/logo_bottom.gif" alt="底部Logo">
<span id="return-top"><b>↑</b></span>
</div>
<script type="text/javascript">
window.siteId = 4;
window.cmsTemplets = "/templets/new";
window.cmsTempletsVer = "1.6.68";
</script>
<script src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/jquery1.js"></script>
<script src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/common.js"></script>
<span style="display:none;"><script src="Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9A%E8%B6%85%E7%BA%A7%E7%AE%80%E5%8D%95%E7%9A%84Python%E7%88%AC%E8%99%AB%E6%95%99%E7%A8%8B_files/z_stat.php" type="text/javascript" defer="defer" async="async"></script></span>

</body></html>